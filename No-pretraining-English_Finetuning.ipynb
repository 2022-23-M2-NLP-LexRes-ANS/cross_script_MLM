{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d1bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tokenizers import trainers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "import conllu\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede986fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "bert_default_vocab = tokenizer.get_vocab().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde584ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train = open(\"UD_English-ParTUT/en_partut-ud-train.conllu\", \"r\", encoding=\"utf-8\") \n",
    "english_test = open(\"UD_English-ParTUT/en_partut-ud-test.conllu\", \"r\", encoding=\"utf-8\") \n",
    "english_dev = open(\"UD_English-ParTUT/en_partut-ud-dev.conllu\", \"r\", encoding=\"utf-8\") \n",
    "\n",
    "eng_train_data = conllu.parse(english_train.read())  # ['id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "eng_train_sents = [[unidecode(token['form']) for token in sentence] for sentence in eng_train_data]\n",
    "eng_u_train_tags = [[token['upostag'] for token in sentence] for sentence in eng_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "502ac595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokenizations(sentences, taggings, tokenizer):  \n",
    "    bert_tokenized_sentences = []\n",
    "    aligned_taggings = []\n",
    "    for sentence, tagging in zip(sentences, taggings):\n",
    "    # first generate BERT-tokenization        \n",
    "        bert_tokenized_sentence = tokenizer.tokenize(' '.join(sentence))\n",
    "\n",
    "        aligned_tagging = []\n",
    "        current_word = ''\n",
    "        index = 0 # index of current word in sentence and tagging\n",
    "        for token in bert_tokenized_sentence:\n",
    "            current_word += re.sub(r'^##', '', token) # recompose word with subtoken\n",
    "            \n",
    "\n",
    "      # note that some word factors correspond to unknown words in BERT\n",
    "            assert token == '[UNK]' or sentence[index].startswith(current_word)  \n",
    "\n",
    "            if token == '[UNK]' or sentence[index] == current_word: # if we completed a word\n",
    "                current_word = ''\n",
    "                aligned_tagging.append(tagging[index])\n",
    "                index += 1\n",
    "            else: # otherwise insert padding\n",
    "                aligned_tagging.append('[PAD]')                \n",
    "            \n",
    "\n",
    "        assert len(bert_tokenized_sentence) == len(aligned_tagging)\n",
    "\n",
    "        bert_tokenized_sentences.append(bert_tokenized_sentence)\n",
    "        aligned_taggings.append(aligned_tagging)\n",
    "\n",
    "    return bert_tokenized_sentences, aligned_taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6476a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_trainedsents, aligned_trainedtags = align_tokenizations(eng_train_sents, eng_u_train_tags, tokenizer)\n",
    "len_tags = len(set(tag for tags in aligned_trainedtags for tag in tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a162e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<pad>'] = 0\n",
    "\n",
    "def convert_to_ids(sentences, taggings):\n",
    "    sentences_ids = []\n",
    "    taggings_ids = []\n",
    "    for sentence, tagging in zip(sentences, taggings):\n",
    "        sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['SEP'])).long()\n",
    "        tagging_tensor = torch.tensor([0] + [label_vocab[tag] for tag in tagging] + [0]).long()\n",
    "\n",
    "        sentences_ids.append(sentence_tensor.to(device))\n",
    "        taggings_ids.append(tagging_tensor.to(device))\n",
    "    return sentences_ids, taggings_ids\n",
    "\n",
    "def collate_fn(items):\n",
    "    max_len = max(len(item[0]) for item in items)\n",
    "\n",
    "    sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
    "    taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
    "\n",
    "    for i, (sentence, tagging) in enumerate(items):\n",
    "        sentences[i][:len(sentence)] = sentence\n",
    "        taggings[i][:len(tagging)] = tagging\n",
    "\n",
    "    return sentences, taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cecd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosTaggingDataset(Dataset):\n",
    "    def __init__(self, sentences, taggings):\n",
    "        assert len(sentences) == len(taggings)\n",
    "        self.sentences = sentences\n",
    "        self.taggings = taggings\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i], self.taggings[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b28eefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataloader = DataLoader(list(zip(aligned_trainedsents, aligned_trainedtags)), batch_size=64, shuffle=True)\n",
    "sent_ids, tag_ids = convert_to_ids(aligned_trainedsents, aligned_trainedtags)\n",
    "train_dataloader = DataLoader(PosTaggingDataset(sent_ids, tag_ids), batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8634d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 151])\n",
      "torch.Size([64, 151])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9ab80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 66])\n",
      "Labels batch shape: torch.Size([64, 66])\n",
      "Label: tensor([ 0,  2,  3,  1,  2,  3,  1,  7,  8,  6,  2,  8, 10,  6,  2,  3,  8,  9,\n",
      "         9,  8,  1,  2,  9,  9,  8,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05d32267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.3657113143375943, 0.03513685297739642)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearProbeBert(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.probe.parameters()\n",
    "  \n",
    "    def forward(self, sentences):\n",
    "        with torch.no_grad(): # no training of BERT parameters\n",
    "            word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "        return self.probe(word_rep)\n",
    "    \n",
    "def perf(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval() # do not apply training-specific steps such as dropout\n",
    "    total_loss = correct = num_loss = num_perf = 0\n",
    "    for x, y in loader:\n",
    "        with torch.no_grad(): # no need to store computation graph for gradients\n",
    "      # perform inference and compute loss\n",
    "            y_scores = model(x)\n",
    "            loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1)) # requires tensors of shape (num-instances, num-labels) and (num-instances)\n",
    "\n",
    "      # gather loss statistics\n",
    "            total_loss += loss.item()\n",
    "            num_loss += 1\n",
    "\n",
    "      # gather accuracy statistics\n",
    "            y_pred = torch.max(y_scores, 2)[1] # compute highest-scoring tag\n",
    "            mask = (y != 0) # ignore <pad> tags\n",
    "            correct += torch.sum((y_pred == y) * mask) # compute number of correct predictions\n",
    "            num_perf += torch.sum(mask).item()\n",
    "    return total_loss / num_loss, correct.item() / num_perf\n",
    "\n",
    "# without training, accuracy should be a bit less than 2% (chance of getting a label correct)\n",
    "perf(LinearProbeBert(len(label_vocab)), train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd0d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
