{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tokenizers import trainers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchsummary import summary\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import conllu\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, path, txt_file, tokenizer, max_length):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.path = path\n",
    "        self.train_set = pd.read_csv(txt_file, delimiter='\\t', header=None, index_col=None)\n",
    "        self.train_set.drop(0, inplace=True, axis=1)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_set)\n",
    "    def __getitem__(self, index):\n",
    "        sent_1 = self.train_set.iloc[index]\n",
    "        # print(sent_1) for debugging\n",
    "        inputs = self.tokenizer.encode_plus(sent_1, truncation=True, max_length=self.max_length, return_attention_mask=True, return_tensors=\"pt\")\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        return {\"ids\": torch.tensor(ids, dtype=torch.long), \"mask\": torch.tensor(mask, dtype=torch.long), \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set clean up. We are using the universal taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Í', '2016', 'væntar', 'oyggin', 'at', 'fáa', 'yvir', 'tíggju', 'milliónir', 'ferðafólk', 'á', 'vitjan', '.']\n",
      "['Pr', 'Num', 'V', 'N', 'IM', 'V', 'Pr', 'Num', 'N', 'N', 'Pr', 'N', 'CLB']\n",
      "['ADP', 'NUM', 'VERB', 'NOUN', 'PART', 'VERB', 'ADP', 'NUM', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "eval_file = open(\"UD_Faroese-OFT/fo_oft-ud-test.conllu\", \"r\", encoding=\"utf-8\") \n",
    "data = conllu.parse(eval_file.read())  # ['id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "sentences = [[token['form'] for token in sentence] for sentence in data]\n",
    "x_taggings = [[token['xpostag'] for token in sentence] for sentence in data]  # weird tags\n",
    "u_taggings = [[token['upostag'] for token in sentence] for sentence in data]\n",
    "\n",
    "# faroese_oft = [sent for sent in conllu.parse_incr(eval_file)]\n",
    "print(sentences[-4])\n",
    "print(x_taggings[-4])\n",
    "print(u_taggings[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb00a803e9a4eb78fcca25934e5f978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d48dad1353b4fe9901a1bacbc8db38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8036b9d76be24c3794819b6ea5a404af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91060845809440d291159feb5bdefb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(119621, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = \"fao_wikipedia_2021_30K-sentences.txt\"\n",
    "\n",
    "f = open(corpus_file, 'r', encoding=\"utf-8\")\n",
    "faroese_Regex = re.compile(r\"^\\d+\\s+\")\n",
    "faroese_sents = [faroese_Regex.sub('', sent) for sent in f.readlines()]  # for faroese\n",
    "punctuation = {0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22, 0x2013:0x2D, 0x2010:0x2D, 0x2014:0x2D, 0x2026:0x85}\n",
    "faroese_sents = [sent.translate(punctuation) for sent in faroese_sents]\n",
    "\n",
    "faroese_words = [sent.split() for sent in faroese_sents]\n",
    "faroese_words = [word for sent in faroese_words for word in sent]\n",
    "f.close()\n",
    "train_corpus = \"\".join(faroese_sents)\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_lower_case=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "long_tokens = list()\n",
    "for w in faroese_words: \n",
    "    single_tokens = tokenizer.tokenize(w)\n",
    "    long_tokens.append((w, single_tokens))\n",
    "\n",
    "longest = [subwords[0] for subwords in long_tokens if len(subwords[1]) >= 11]  # try segment from 10 or higher and clean web addresses as well as () and '\n",
    "# 12 subtokens == 44 words\n",
    "# 11 subtokens == 96 words\n",
    "\n",
    "bert_vocab = tokenizer.get_vocab().keys()\n",
    "words_in_bert = set([word in bert_vocab for word in longest])\n",
    "longest = [word for word in longest if word[:4] != \"http\"]\n",
    "foreign_tokensRegex = re.compile(r\"^(\\(|'|\\\")\")\n",
    "subword_tokens = [word for word in longest if not foreign_tokensRegex.search(word)] \n",
    "special_tokens_dict = {'additional_special_tokens': subword_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Resizing\n",
    "\n",
    "bert_model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "# dataset = BERTDataset('.', corpus_file, tokenizer, max_length=100)\n",
    "# dataloader = DataLoader(dataset=dataset, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing whole text UNNEEDED\n",
    "\n",
    "#text = pd.read_csv()\n",
    "#tokenised_text = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokenizations(sentences, taggings):  # https://pageperso.lis-lab.fr/benoit.favre/pstaln/09_embedding_evaluation.html\n",
    "    bert_tokenized_sentences = []\n",
    "    aligned_taggings = []\n",
    "\n",
    "    for sentence, tagging in zip(sentences, taggings):\n",
    "    # first generate BERT-tokenization        \n",
    "        bert_tokenized_sentence = tokenizer.tokenize(' '.join(sentence))\n",
    "\n",
    "        aligned_tagging = []\n",
    "        current_word = ''\n",
    "        index = 0 # index of current word in sentence and tagging\n",
    "        for token in bert_tokenized_sentence:\n",
    "            current_word += re.sub(r'^##', '', token) # recompose word with subtoken\n",
    "            #sentence[index] = sentence[index].replace('\\xad', '') # fix bug in data\n",
    "\n",
    "      # note that some word factors correspond to unknown words in BERT\n",
    "            assert token == '[UNK]' or sentence[index].startswith(current_word)  \n",
    "\n",
    "            if token == '[UNK]' or sentence[index] == current_word: # if we completed a word\n",
    "                current_word = ''\n",
    "                aligned_tagging.append(tagging[index])\n",
    "                index += 1\n",
    "            else: # otherwise insert padding\n",
    "                aligned_tagging.append('[PAD]')                \n",
    "            \n",
    "\n",
    "        assert len(bert_tokenized_sentence) == len(aligned_tagging)\n",
    "\n",
    "        bert_tokenized_sentences.append(bert_tokenized_sentence)\n",
    "        aligned_taggings.append(aligned_tagging)\n",
    "\n",
    "    return bert_tokenized_sentences, aligned_taggings\n",
    "\n",
    "aligned_sents, aligned_tags = align_tokenizations(sentences, u_taggings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Før', '##oya', '##r', 'eru', 'ikki', 'li', '##mur', 'í', 'ES', ',', 'hóa', '##st', 'Danmark', 'er', '.']\n",
      "tensor([   101,  58997,  51814,  10129,  18098, 106434,  11614,  33246,    267,\n",
      "         49004,    117,  20105,  10562,  17271,  10163,    119,    100],\n",
      "       device='cuda:0')\n",
      "['[PAD]', '[PAD]', 'PROPN', 'AUX', 'ADV', '[PAD]', 'NOUN', 'ADP', 'NOUN', 'PUNCT', '[PAD]', 'SCONJ', 'PROPN', 'VERB', 'PUNCT']\n",
      "tensor([0, 1, 1, 2, 3, 4, 1, 5, 6, 5, 7, 1, 8, 2, 9, 7, 0], device='cuda:0')\n",
      "\n",
      "['Av', '##hal', '##ds', '##rør', '##sla', '##n', 'byrja', '##r', '.']\n",
      "tensor([  101, 33799, 18453, 13268, 93664, 61432, 10115, 98195, 10129,   119,\n",
      "          100], device='cuda:0')\n",
      "['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'NOUN', '[PAD]', 'VERB', 'PUNCT']\n",
      "tensor([0, 1, 1, 1, 1, 1, 5, 1, 9, 7, 0], device='cuda:0')\n",
      "num labels: 18\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<pad>'] = 0\n",
    "\n",
    "def convert_to_ids(sentences, taggings):\n",
    "    sentences_ids = []\n",
    "    taggings_ids = []\n",
    "    for sentence, tagging in zip(sentences, taggings):\n",
    "        sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['SEP'])).long()\n",
    "        tagging_tensor = torch.tensor([0] + [label_vocab[tag] for tag in tagging] + [0]).long()\n",
    "\n",
    "        sentences_ids.append(sentence_tensor.to(device))\n",
    "        taggings_ids.append(tagging_tensor.to(device))\n",
    "    return sentences_ids, taggings_ids\n",
    "\n",
    "\n",
    "sent_ids, tag_ids = convert_to_ids(aligned_sents, aligned_tags)\n",
    "#train_sentences_ids, train_taggings_ids = convert_to_ids(train_bert_tokenized_sentences, train_aligned_taggings)\n",
    "#valid_sentences_ids, valid_taggings_ids = convert_to_ids(valid_bert_tokenized_sentences, valid_aligned_taggings)\n",
    "#test_sentences_ids, test_taggings_ids = convert_to_ids(test_bert_tokenized_sentences, test_aligned_taggings)\n",
    "print(aligned_sents[0])\n",
    "print(sent_ids[0])  \n",
    "print(aligned_tags[0])\n",
    "print(tag_ids[0])  \n",
    "print()\n",
    "print(aligned_sents[1])\n",
    "print(sent_ids[1])  \n",
    "print(aligned_tags[1])\n",
    "print(tag_ids[1])  \n",
    "\n",
    "print('num labels:', len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3])\n",
      "tensor([[4, 5, 6],\n",
      "        [3, 4, 0]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(items):\n",
    "    max_len = max(len(item[0]) for item in items)\n",
    "\n",
    "    sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
    "    taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
    "\n",
    "    for i, (sentence, tagging) in enumerate(items):\n",
    "        sentences[i][0:len(sentence)] = sentence\n",
    "        taggings[i][0:len(tagging)] = tagging\n",
    "\n",
    "    return sentences, taggings\n",
    "\n",
    "x, y = collate_fn([[torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])], [torch.tensor([1, 2]), torch.tensor([3, 4])]])\n",
    "print(x.shape, y.shape)\n",
    "print(y)\n",
    "print()\n",
    "#x, y = collate_fn(sent_ids, tag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PosTaggingDataset(Dataset):\n",
    "    def __init__(self, sentences, taggings):\n",
    "        assert len(sentences) == len(taggings)\n",
    "        self.sentences = sentences\n",
    "        self.taggings = taggings\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i], self.taggings[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,   151, 11637,  ...,     0,     0,     0],\n",
      "        [  101, 33884, 10661,  ...,     0,     0,     0],\n",
      "        [  101, 36702, 36767,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   157,   119,  ...,     0,     0,     0],\n",
      "        [  101,   148, 26776,  ...,     0,     0,     0],\n",
      "        [  101, 51874, 20084,  ...,     0,     0,     0]], device='cuda:0') tensor([[0, 1, 4,  ..., 0, 0, 0],\n",
      "        [0, 1, 6,  ..., 0, 0, 0],\n",
      "        [0, 1, 9,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 0, 0, 0],\n",
      "        [0, 1, 1,  ..., 0, 0, 0],\n",
      "        [0, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "eval_loader = DataLoader(PosTaggingDataset(sent_ids, tag_ids), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 18])\n"
     ]
    }
   ],
   "source": [
    "class LinearProbeBert(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.probe.parameters()\n",
    "  \n",
    "    def forward(self, sentences):\n",
    "        with torch.no_grad(): # no training of BERT parameters\n",
    "            word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "        return self.probe(word_rep)\n",
    "\n",
    "# the model should return a tensor of shape (batch size, sequence length, number of labels)\n",
    "bert_model = LinearProbeBert(len(label_vocab))\n",
    "y = bert_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class fineBERT(nn.Module): \n",
    "    def __init__(self, num_labels):\n",
    "        super(fineBERT, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.probe = nn.Linear(self.bert_model.config.hidden_size, num_labels)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.bert_model(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        return self.out(output)\n",
    "    \n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = fineBERT(len(label_vocab))\n",
    "model.cuda()\n",
    "#optimizer= torch.optim.Adam(model.parameters(),lr= 0.0001)\n",
    "\n",
    "for param in model.bert_model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fineBERT(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (probe): Linear(in_features=768, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
