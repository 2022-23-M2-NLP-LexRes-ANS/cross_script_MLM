{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4066b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download arabic wikipedia extract\n",
    "\n",
    "# Extract it with WikiExtractor\n",
    "\n",
    "# Romanize it with uroman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import contextlib\n",
    "import hashlib\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import filehash\n",
    "import fire\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import sarge\n",
    "import torch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ef618b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T16:45:41.679352Z",
     "start_time": "2023-02-09T16:45:41.675911Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging as logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee48e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:24:42.494687Z",
     "start_time": "2023-02-09T15:24:42.488061Z"
    }
   },
   "outputs": [],
   "source": [
    "def _download_file_maybe(url: str, out_file_path=None):\n",
    "    \"\"\"NOTE: FUNCTION NOT WORKING.\n",
    "    \n",
    "    Download a file unless a file already exists at destination path. In all\n",
    "    cases, log a digest with checksum of the file that is present.\"\"\"\n",
    "    if out_file_path is None:\n",
    "        out_file_path = Path('.') / Path(Path(url).name)\n",
    "\n",
    "    if out_file_path.is_file():\n",
    "        logger.warning(\n",
    "            \"Not downloading because there is already a file at: {} ({})\".format(\n",
    "                out_file_path, out_file_path.absolute()\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\n",
    "            \"Now we are going to download the COHA sample corpus from the internet\"\n",
    "        )\n",
    "        # https://stackoverflow.com/questions/1078524/how-to-specify-the-download-location-with-wget\n",
    "        command = sarge.shell_format(\"wget -q -O {} {} \", out_file_path, url)\n",
    "        # logger.info(f'Running {command=}')\n",
    "        _run_cmd(command, cmd_label=\"wget\")\n",
    "\n",
    "    logger.debug(f\"{file_info_digest(out_file_path)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf776d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:38:47.054384Z",
     "start_time": "2023-02-09T15:38:47.046389Z"
    }
   },
   "source": [
    "## Download arabic wikipedia extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6953fac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:38:47.054384Z",
     "start_time": "2023-02-09T15:38:47.046389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://dumps.wikimedia.org/arwiki/20230201/\\n\\nSee instructions here: https://github.com/attardi/wikiextractor\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NO! # https://dumps.wikimedia.org/arwiki/latest/arwiki-latest-pages-articles.xml.bz2 \n",
    "# YES # https://dumps.wikimedia.org/arwiki/20230201/arwiki-20230201-pages-articles.xml.bz2  \n",
    "\n",
    "# https://dumps.wikimedia.org/arwiki/20230201/arwiki-20230201-sha1sums.txt \n",
    "\n",
    "'''\n",
    "https://dumps.wikimedia.org/arwiki/20230201/\n",
    "\n",
    "See instructions here: https://github.com/attardi/wikiextractor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f009d20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:37:47.084367Z",
     "start_time": "2023-02-09T15:37:45.445628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish: Unknown command: fish_add_path\n",
      "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish (line 34): \n",
      "fish_add_path --prepend --global \"/nix/var/nix/profiles/default/bin\"\n",
      "^\n",
      "from sourcing file /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish\n",
      "\tcalled on line 185 of file /usr/share/fish/config.fish\n",
      "in function '.' with arguments '/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish'\n",
      "\tcalled on line 4 of file /etc/fish/conf.d/nix.fish\n",
      "from sourcing file /etc/fish/conf.d/nix.fish\n",
      "\tcalled on line 294 of file /usr/share/fish/config.fish\n",
      "from sourcing file /usr/share/fish/config.fish\n",
      "\tcalled during startup\n",
      "fish: Unknown command: fish_add_path\n",
      "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish (line 35): \n",
      "fish_add_path --prepend --global \"$HOME/.nix-profile/bin\"\n",
      "^\n",
      "from sourcing file /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish\n",
      "\tcalled on line 185 of file /usr/share/fish/config.fish\n",
      "in function '.' with arguments '/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish'\n",
      "\tcalled on line 4 of file /etc/fish/conf.d/nix.fish\n",
      "from sourcing file /etc/fish/conf.d/nix.fish\n",
      "\tcalled on line 294 of file /usr/share/fish/config.fish\n",
      "from sourcing file /usr/share/fish/config.fish\n",
      "\tcalled during startup\n",
      "--2023-02-09 16:37:46--  https://dumps.wikimedia.org/arwiki/20230201/arwiki-20230201-sha1sums.txt\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142, 2620:0:861:2:208:80:154:142\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19150 (19K) [text/plain]\n",
      "Saving to: ‘arwiki-20230201-sha1sums.txt’\n",
      "\n",
      "arwiki-20230201-sha 100%[===================>]  18.70K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-09 16:37:46 (42.4 MB/s) - ‘arwiki-20230201-sha1sums.txt’ saved [19150/19150]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://dumps.wikimedia.org/arwiki/20230201/arwiki-20230201-sha1sums.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba78529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:37:45.432228Z",
     "start_time": "2023-02-09T15:32:16.845682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish: Unknown command: fish_add_path\n",
      "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish (line 34): \n",
      "fish_add_path --prepend --global \"/nix/var/nix/profiles/default/bin\"\n",
      "^\n",
      "from sourcing file /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish\n",
      "\tcalled on line 185 of file /usr/share/fish/config.fish\n",
      "in function '.' with arguments '/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish'\n",
      "\tcalled on line 4 of file /etc/fish/conf.d/nix.fish\n",
      "from sourcing file /etc/fish/conf.d/nix.fish\n",
      "\tcalled on line 294 of file /usr/share/fish/config.fish\n",
      "from sourcing file /usr/share/fish/config.fish\n",
      "\tcalled during startup\n",
      "fish: Unknown command: fish_add_path\n",
      "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish (line 35): \n",
      "fish_add_path --prepend --global \"$HOME/.nix-profile/bin\"\n",
      "^\n",
      "from sourcing file /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish\n",
      "\tcalled on line 185 of file /usr/share/fish/config.fish\n",
      "in function '.' with arguments '/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish'\n",
      "\tcalled on line 4 of file /etc/fish/conf.d/nix.fish\n",
      "from sourcing file /etc/fish/conf.d/nix.fish\n",
      "\tcalled on line 294 of file /usr/share/fish/config.fish\n",
      "from sourcing file /usr/share/fish/config.fish\n",
      "\tcalled during startup\n",
      "--2023-02-09 16:32:17--  https://dumps.wikimedia.org/arwiki/20230201/arwiki-20230201-pages-articles.xml.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142, 2620:0:861:2:208:80:154:142\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1481301661 (1.4G) [application/octet-stream]\n",
      "Saving to: ‘arwiki-20230201-pages-articles.xml.bz2’\n",
      "\n",
      "arwiki-20230201-pag 100%[===================>]   1.38G  4.32MB/s    in 5m 27s  \n",
      "\n",
      "2023-02-09 16:37:45 (4.32 MB/s) - ‘arwiki-20230201-pages-articles.xml.bz2’ saved [1481301661/1481301661]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#! wget https://dumps.wikimedia.org/arwiki/20230201/arwiki-20230201-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377a8f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:41:13.247791Z",
     "start_time": "2023-02-09T15:41:08.665444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish: Unknown command: fish_add_path\n",
      "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish (line 34): \n",
      "fish_add_path --prepend --global \"/nix/var/nix/profiles/default/bin\"\n",
      "^\n",
      "from sourcing file /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish\n",
      "\tcalled on line 185 of file /usr/share/fish/config.fish\n",
      "in function '.' with arguments '/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish'\n",
      "\tcalled on line 4 of file /etc/fish/conf.d/nix.fish\n",
      "from sourcing file /etc/fish/conf.d/nix.fish\n",
      "\tcalled on line 294 of file /usr/share/fish/config.fish\n",
      "from sourcing file /usr/share/fish/config.fish\n",
      "\tcalled during startup\n",
      "fish: Unknown command: fish_add_path\n",
      "/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish (line 35): \n",
      "fish_add_path --prepend --global \"$HOME/.nix-profile/bin\"\n",
      "^\n",
      "from sourcing file /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish\n",
      "\tcalled on line 185 of file /usr/share/fish/config.fish\n",
      "in function '.' with arguments '/nix/var/nix/profiles/default/etc/profile.d/nix-daemon.fish'\n",
      "\tcalled on line 4 of file /etc/fish/conf.d/nix.fish\n",
      "from sourcing file /etc/fish/conf.d/nix.fish\n",
      "\tcalled on line 294 of file /usr/share/fish/config.fish\n",
      "from sourcing file /usr/share/fish/config.fish\n",
      "\tcalled during startup\n",
      "arwiki-20230201-pages-articles.xml.bz2: OK\n"
     ]
    }
   ],
   "source": [
    "! sha1sum --ignore-missing --check arwiki-20230201-sha1sums.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d62449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:57:51.527142Z",
     "start_time": "2023-02-09T15:57:51.521258Z"
    }
   },
   "source": [
    "## Extract it with WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c492d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T15:57:51.527142Z",
     "start_time": "2023-02-09T15:57:51.521258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith template procesing included:\\n\\n...\\nINFO: Preprocessed 3400000 pages\\nINFO: Loaded 126172 templates in 425.4s\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NOTE: wikiextractor will not work with python 3.11. It will work with 3.10.\n",
    "\n",
    "Option --no-templates significantly speeds up the extractor, avoiding the cost of expanding MediaWiki templates.\n",
    "'''\n",
    "# ! time python -m wikiextractor.WikiExtractor arwiki-20230201-pages-articles.xml.bz2 \n",
    "\n",
    "'''\n",
    "With template processing included:\n",
    "\n",
    "...\n",
    "INFO: Preprocessed 3400000 pages\n",
    "INFO: Loaded 126172 templates in 425.4s\n",
    "'''\n",
    "\n",
    "# ! time python -m wikiextractor.WikiExtractor --no-templates arwiki-20230201-pages-articles.xml.bz2   \n",
    "\n",
    "'''With --no-templates, we jump immediately to the extraction step (no preprocessing pages).'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a549a4",
   "metadata": {},
   "source": [
    "```bash\n",
    "╭─user at s-m2tal in ⌁/cours/LexResCerisara/ANSProj/faroese_mBERT (scott ●3…104)\n",
    "╰─λ time python -m wikiextractor.WikiExtractor --no-templates arwiki-20230201-pages-articles.xml.bz2                                    (LexResProject) 0 (0.001s) < 16:59:17\n",
    "INFO: Starting page extraction from arwiki-20230201-pages-articles.xml.bz2.\n",
    "INFO: Using 1 extract processes.\n",
    "INFO: Extracted 100000 articles (1094.8 art/s)\n",
    "INFO: Extracted 200000 articles (1673.1 art/s)\n",
    "INFO: Extracted 300000 articles (1844.7 art/s)\n",
    "INFO: Extracted 400000 articles (1650.6 art/s)\n",
    "INFO: Extracted 500000 articles (1812.1 art/s)\n",
    "INFO: Extracted 600000 articles (1985.7 art/s)\n",
    "INFO: Extracted 700000 articles (3096.2 art/s)\n",
    "INFO: Extracted 800000 articles (2146.3 art/s)\n",
    "INFO: Extracted 900000 articles (2016.3 art/s)\n",
    "INFO: Extracted 1000000 articles (1978.5 art/s)\n",
    "INFO: Extracted 1100000 articles (1490.3 art/s)\n",
    "INFO: Extracted 1200000 articles (2329.1 art/s)\n",
    "INFO: Extracted 1300000 articles (2531.2 art/s)\n",
    "INFO: Extracted 1400000 articles (2942.8 art/s)\n",
    "INFO: Extracted 1500000 articles (3129.7 art/s)\n",
    "INFO: Extracted 1600000 articles (2685.5 art/s)\n",
    "INFO: Extracted 1700000 articles (3340.0 art/s)\n",
    "INFO: Extracted 1800000 articles (2942.4 art/s)\n",
    "INFO: Extracted 1900000 articles (1992.6 art/s)\n",
    "INFO: Extracted 2000000 articles (1589.1 art/s)\n",
    "INFO: Extracted 2100000 articles (1736.0 art/s)\n",
    "INFO: Extracted 2200000 articles (1526.2 art/s)\n",
    "INFO: Finished 1-process extraction of 2219910 articles in 1115.8s (1989.6 art/s)\n",
    "\n",
    "________________________________________________________\n",
    "Executed in   18.60 mins   fish           external \n",
    "   usr time  1541.75 secs  357.00 micros  1541.75 secs \n",
    "   sys time   87.66 secs  135.00 micros   87.66 secs \n",
    "\n",
    "╭─user at s-m2tal in ⌁/cours/LexResCerisara/ANSProj/faroese_mBERT (scott ●3✚1…2261)\n",
    "╰─λ                                                                                                                                  (LexResProject) 0 (18:35.847) < 17:18:1100000\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841b83a",
   "metadata": {},
   "source": [
    "https://github.com/attardi/wikiextractor/wiki/File-Format\n",
    "\n",
    "\"\n",
    "Document files contains a series of Wikipedia articles, represented each by an XML <tt>doc</tt> element: ... ... ... ...\n",
    "\n",
    "The element <tt>doc</tt> has the following attributes:\n",
    "\n",
    "    <tt>id</tt>, which identifies the document by means of a unique serial number\n",
    "    <tt>url</tt>, which provides the URL of the original Wikipedia page.\n",
    "\n",
    "The content of a <tt>doc</tt> element consists of pure text, one sentence per line.\n",
    "\n",
    "Here is an example of a <tt>doc</tt> element:\n",
    "Harmonium. L'harmonium è uno strumento musicale azionato con una tastiera, detta manuale. Sono stati costruiti anche alcuni harmonium con due manuali. ...\n",
    "\n",
    "Notice that because of Wikipedia conventions, the first sentence is the title of the article.\n",
    "\n",
    "Such documents are produced by Wikipedia Extractor .\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f1d60",
   "metadata": {},
   "source": [
    "## Romanize it with uroman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a50bf7",
   "metadata": {},
   "source": [
    "Cf. https://github.com/isi-nlp/uroman \n",
    "\n",
    "...\n",
    "\n",
    "### Usage\n",
    "```bash\n",
    "$ uroman.pl [-l <lang-code>] [--chart] [--no-cache] < STDIN\n",
    "       where the optional <lang-code> is a 3-letter languages code, e.g. ara, bel, bul, deu, ell, eng, fas,\n",
    "            grc, ell, eng, heb, kaz, kir, lav, lit, mkd, mkd2, oss, pnt, pus, rus, srp, srp2, tur, uig, ukr, yid.\n",
    "       --chart specifies chart output (in JSON format) to represent alternative romanizations.\n",
    "       --no-cache disables caching.\n",
    "```\n",
    "### Examples\n",
    "```bash\n",
    "$ bin/uroman.pl < text/zho.txt\n",
    "$ bin/uroman.pl -l tur < text/tur.txt\n",
    "$ bin/uroman.pl -l heb --chart < text/heb.txt\n",
    "$ bin/uroman.pl < test/multi-script.txt > test/multi-script.uroman.txt\n",
    "```\n",
    "\n",
    "Identifying the input as Arabic, Belarusian, Bulgarian, English, Farsi, German,\n",
    "Ancient Greek, Modern Greek, Pontic Greek, Hebrew, Kazakh, Kyrgyz, Latvian,\n",
    "Lithuanian, North Macedonian, Russian, Serbian, Turkish, Ukrainian, Uyghur or \n",
    "Yiddish will improve romanization for those languages as some letters in those \n",
    "languages have different sound values from other languages using the same script \n",
    "(French, Russian, Hebrew respectively).\n",
    "No effect for other languages in this version.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b78f8",
   "metadata": {},
   "source": [
    "script uroman-quick.pl for Arabic script languages, incl. Uyghur.\n",
    "   Much faster, pre-caching mapping of Arabic to Latin characters, simple greedy processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! uroman.pl -l ara   < STDIN   > BLA.uroman.txt\n",
    "\n",
    "# !  ./uroman.pl -l ara   < ~/cours/LexResCerisara/ANSProj/faroese_mBERT/text/AA/wiki_00   | less   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63632578",
   "metadata": {},
   "source": [
    "```bash\n",
    "╭─user at s-m2tal in ⌁/cours/LexResCerisara/ANSProj/uroman/bin (master ✔)\n",
    "╰─λ ./uroman.pl -l ara   < ~/cours/LexResCerisara/ANSProj/faroese_mBERT/text/AA/wiki_00   | tail                     \n",
    "\n",
    "tkhtlf aljam'a al'rbya akhtlafa kbyra 'n alathad alawrwby, flm thqq aljam'a al'rbya mqdara mlhwza mn altkaml aliqlymy, wlys lljam'a 'laqa mbashra m' aldwl ala'da'. wlkn aljam'a al'rbya mbnya 'la mbadye td'm wtrwj lqwmya 'rbya mwhda wtwhyd mwaqf aldwl ala'da' bkhsws mkhtlf alqdaya.\n",
    "jmy' a'da' aljam'a al'rbya a'da' fy mnzma almwtmr alislamy. kma an hnak mjmw'at fr'ya fy aljam'a, mthl «mjls alt'awn alkhlyjy» w«athad almghrb al'rby».\n",
    "qayema alqmm al'rbya.\n",
    "mnth tasys aljam'a al'rbya fy 'am 1945 'qd alqada al'rb 39 ajtma' qma, bynha 25 qma 'adya w11 qmm taryea, ila janb thlatha qmm aqtsadya:\n",
    "alantqadat.\n",
    "in jam'a aldwl al'rbya lys fqt twajh alantqadat mn aldwl alakhra\n",
    "bl inha lys lha sh'bya lda m'zm alsh'wb al'rbya bsbb anha fshlt fy hl alkthyr mn alqdaya al'rbya wl'l abrzha alsra' al'rby alisrayeyly wahtlal alkyan alshywny llarady alflstynya 'am 1948 w'am 1967\n",
    "walkthyr mn alqdaya kalazma alswrya whl mshkla alsh'b alswry althy asbh y'any alljw' waltshryd fy aldwl almjawra walghyr mjawra 'da anh ywajh almwt mn jmy' atraf alnza' dakhl wtnh swrya.\n",
    "\n",
    "</doc>\n",
    "\n",
    "╭─user at s-m2tal in ⌁/cours/LexResCerisara/ANSProj/uroman/bin (master ✔)\n",
    "                                                                                     \n",
    "(base) 0 (10.730s) < 17:26:52\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "╭─user at s-m2tal in ⌁/cours/LexResCerisara/ANSProj/uroman/bin (master ✔)\n",
    "╰─λ ./uroman-quick.pl -l ara   < ~/cours/LexResCerisara/ANSProj/faroese_mBERT/text/AA/wiki_00   | tail               \n",
    "\n",
    "..\n",
    "tkhtlf aljam'a al'rbya akhtlafa kbyra 'n alathad alawrwby, flm thqq aljam'a al'rbya mqdara mlhwza mn altkaml aliqlymy, wlys lljam'a 'laqa mbashra m' aldwl ala'da'. wlkn aljam'a al'rbya mbnya 'la mbadye td'm wtrwj lqwmya 'rbya mwhda wtwhyd mwaqf aldwl ala'da' bkhsws mkhtlf alqdaya.\n",
    "jmy' a'da' aljam'a al'rbya a'da' fy mnzma almwtmr alislamy. kma an hnak mjmw'at fr'ya fy aljam'a, mthl «mjls alt'awn alkhlyjy» w«athad almghrb al'rby».\n",
    "qayema alqmm al'rbya.\n",
    "mnth tasys aljam'a al'rbya fy 'am 1945 'qd alqada al'rb 39 ajtma' qma, bynha 25 qma 'adya w11 qmm taryea, ila janb thlatha qmm aqtsadya:\n",
    "alantqadat.\n",
    "in jam'a aldwl al'rbya lys fqt twajh alantqadat mn aldwl alakhra\n",
    "bl inha lys lha sh'bya lda m'zm alsh'wb al'rbya bsbb anha fshlt fy hl alkthyr mn alqdaya al'rbya wl'l abrzha alsra' al'rby alisrayeyly wahtlal alkyan alshywny llarady alflstynya 'am 1948 w'am 1967\n",
    "walkthyr mn alqdaya kalazma alswrya whl mshkla alsh'b alswry althy asbh y'any alljw' waltshryd fy aldwl almjawra walghyr mjawra 'da anh ywajh almwt mn jmy' atraf alnza' dakhl wtnh swrya.\n",
    "\n",
    "</doc>\n",
    "╭─user at s-m2tal in ⌁/cours/LexResCerisara/ANSProj/uroman/bin (master ✔)\n",
    "╰─λ                                                                                                                 \n",
    "\n",
    "(base) 0 (3.287s) < 17:29:39\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4b064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T16:43:16.598492Z",
     "start_time": "2023-02-09T16:43:16.593877Z"
    }
   },
   "source": [
    "\n",
    "This output so far looks pretty good. Remaining tasks to do:\n",
    "\n",
    "- remove wikitext code like `[[` etc\n",
    "- for extractfile in files: for doc in extractfile: append doc to big concat file of full corpus\n",
    "- *maybe* also filter corpus (e.g. X number of docs from each file; randomly select which docs)\n",
    "- Q: should we run uroman on the full dataset after all wikitext stripping and concat; \n",
    "    or on each doc as we feed it in?; or at some other point in pipeline?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245b9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LexResProject]",
   "language": "python",
   "name": "conda-env-LexResProject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
