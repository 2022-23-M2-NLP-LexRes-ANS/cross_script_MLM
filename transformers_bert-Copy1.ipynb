{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import conllu\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and BERT\n",
    "\n",
    "In 2017, a paper was published that revolutionised deep learning for NLP: [Attention is All you Need](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) by Vaswani and others.\n",
    "This paper described a new way of encoding text using a neural architecture called a transformer.\n",
    "It essentially ended up replacing recurrent neural networks (RNNs).\n",
    "\n",
    "The problem with RNNs is that they are inherently sequential.\n",
    "So regardless of how powerful your computer is, if you want to encode a sentence with $n$ words then you will need to process $n$ sequential time steps.\n",
    "\n",
    "The transformer solves this problem by doing something similar to taking the average vector of the embedding vectors.\n",
    "Adding all the embedding vectors together can be done in parallel, so that would solve the RNN's problem.\n",
    "Unfortunately it also loses word order information, which is a big deal.\n",
    "We'll get to how word order information is preserved in a bit, but for now we should focus on the rest of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries, keys, and values\n",
    "\n",
    "Transformers do the following:\n",
    "\n",
    "* Take each word vector ($w_i$) and use three separate neural layers to transform each word vector into three separate vectors: a query vector ($q_i$), a key vector ($k_i$), and a value vector ($v_i$).\n",
    "* Combine each $q_i$ with each $k_j$ (including the key from the same word as $q_i$) to produce a value $a_{ij}$ called an attention value.\n",
    "* Multiply $a_{ij}$ by $v_i$ (the value vector that came from the same word as the key vector) and take their sum to produce a vector for word $j$.\n",
    "\n",
    "Here is a diagram illustrating this architecture:\n",
    "\n",
    "![](attention_full.png)\n",
    "\n",
    "Here is the same diagram but focusing only on the first word's output:\n",
    "\n",
    "![](attention_focused.png)\n",
    "\n",
    "The attention value $ij$ is a number that determines how relevant a particular key $j$ is to a particular query $i$.\n",
    "The more relevant it is, the higher the attention value and the more word $j$ will contribute to the context vector of word $i$.\n",
    "The attention values of a particular query are passed through a softmax function in order to make them all positive and sum to 1.\n",
    "Since each context vector consists of the sum of value vectors multiplied by attention values that sum to 1, the context vector will be a weighted average of value vectors.\n",
    "\n",
    "Essentially, we're comparing each word $w_i$ to each other word $w_j$, including $w_i$ itself, and then determining how important $w_j$ is for $w_i$.\n",
    "This importance is used to make a weighted average of all the words in the sequence in order to represent the meaning of $w_i$.\n",
    "The way we compare $w_i$ with $w_j$ is by comparing the query vector of $w_i$ to the key vector of $w_j$, sort of like how a query is used to search for a key in a database.\n",
    "You can think of the value vector as the value that is returned by the database after making the query, or the value associated with the key in a Python dictionary.\n",
    "This process is done for each word, which happens all in parallel.\n",
    "The advantage of having separate query and key vectors is that you can compare a word to itself without comparing a vector to itself.\n",
    "\n",
    "The attention maker consists of just the dot product of the query and key vectors, which is very fast as you can get the dot product of a bunch of vectors with every vector in another bunch by doing a single matrix multiplication (if each bunch of vectors is made into a matrix).\n",
    "In order to avoid the dot product getting too big with large vectors, the dot product is divided by the square root of the vector size.\n",
    "\n",
    "Transformers generally do not stop here but complicate themselves a little more:\n",
    "\n",
    "* First of all, there isn't just one weighted average of value vectors as shown above but several, which is called multi-head attention.\n",
    "    The process of multi-head attention consists of simply splitting each incoming vector into a number of smaller 'head' vectors which are then processed separately into different queries, keys, and values.\n",
    "    The results are then concatented together into a single large vector which is passed through a feedforward layer.\n",
    "* Secondly, on top of the weighted average of value vectors there is also a vector normalisation layer (converts each vector into a zero-mean unit-variance vector).\n",
    "    In addition, the incoming vectors are added to the result of each layer in order to create residual connections the reduce the effect of vanishing gradients.\n",
    "* We should also keep in mind that there isn't just a single layer of this, but several layers repeating the same process.\n",
    "\n",
    "Finally, it is important to keep in mind that, since we're generating an attention value for each pairing of words, the amount of memory needed is quadratic (squared) to the number of words, which is the price we pay for being able to process everything in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The embeddings\n",
    "\n",
    "The above description suggests that there is no preservation of word order information in the transformer.\n",
    "As-is, when calculating the attention, there is no way to take into account where a particular query and key pair are situated in the sentence, which makes it equivalent to processing a bag of words vector.\n",
    "\n",
    "To avoid this, the word embedding vectors are modified to include positional information by adding to each vector a \"positional embedding vector\".\n",
    "A positional embedding vector is like a word embedding vector but for positions instead of tokens.\n",
    "There are many ways to do this, but one way is by using a positional embedding matrix, that is, having a matrix with a row for each possible position, where each row would be a positional embedding vector, similar to the word embedding matrix.\n",
    "The problem with this is that, just like using a word embedding matrix implies having a fixed number of words in your vocabulary, you'll also need to have a fixed number of positions, which means that you have a maximum sentence length you can process.\n",
    "This is a normal issue in transformers, but we'll generally run out of memory beyond a certain number of words anyway.\n",
    "\n",
    "Many transformers also allow you to include 'token type embeddings', which are used to specify extra information about the words.\n",
    "For example, if you want to input two sentences into the transformer (to do things like predict if one sentence contradicts the other), you would concatenate the two sentences together into a single sequence and use token type embeddings to specify if a word belongs to the first sentence or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence representation\n",
    "\n",
    "We've seen how to represent each word in context, but how do you represent a whole sentence with one vector?\n",
    "A lot of papers to do so by just taking the average of the word vectors.\n",
    "Another way is to add a pseudo-token to the beginning of each sentence, usually called a 'class token', whose context vector is then used to represent the whole sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace and BERT\n",
    "\n",
    "For most tasks, you're better off using a pre-trained transformer that turns words in context vectors.\n",
    "One of the most popular pre-trained transformers is [BERT](https://aclanthology.org/N19-1423/) (Bidirectional Encoder Representations from Transformers).\n",
    "This transformer was trained by self-supervision on a lot of text (mainly from books and Wikipedia) to do two things: predict the missing word from a sentence and predict if two sentences follow each other.\n",
    "The first task was trained by using the pseudo-token \"\\[MASK\\]\" which stands for a missing token. \n",
    "The second was trained by using two pseudo-tokens: \"\\[SEP\\]\" (separator) and \"\\[CLS\\]\" (class) where the first is placed in between two sentences which are concatenated together into a single sequence whilst the context vector of the second is passed into a classifier to determine if the two sentences follow each other or if they were picked randomly.\n",
    "\n",
    "Since it's pre-trained, BERT has its own vocabulary that you need to use together with its own tokeniser.\n",
    "The vocabulary consists of \"word pieces\" which is a solution to unknown tokens.\n",
    "Basically, it is possible to break down each individual character in a word into separate tokens, which would make the vocabulary be all the characters in the alphabet, digits, punctuation, and so on.\n",
    "But this would make the sequences longer than they need to be which requires more memory.\n",
    "So in addition to the individual characters, the vocabulary also includes commonly occuring substrings that are found in words.\n",
    "BERT's tokeniser automatically identifies these substrings and makes them a single token.\n",
    "Any unknown substrings are broken into individual characters and treated as separate tokens.\n",
    "The tokeniser does not include the space character as a token.\n",
    "Instead, each token has two versions: the first token in a word and an inner token.\n",
    "Each of these versions have different indexes in the vocabulary, which allows BERT to know where each word begins and ends.\n",
    "\n",
    "To use BERT, you can use [HuggingFace](https://huggingface.co/models), a library of pre-trained transformers that are readily usable and downloadable.\n",
    "To use HuggingFace you just need to install the \"transformers\" Python library using pip.\n",
    "Let's try using this library to make use of BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to download the pre-trained tokeniser and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "bert_default_vocab = tokenizer.get_vocab().keys()\n",
    "\n",
    "#!pip install allentune --upgrade weird, perhaps dont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train = open(\"UD_English-ParTUT/en_partut-ud-train.conllu\", \"r\", encoding=\"utf-8\") \n",
    "english_test = open(\"UD_English-ParTUT/en_partut-ud-test.conllu\", \"r\", encoding=\"utf-8\") \n",
    "english_dev = open(\"UD_English-ParTUT/en_partut-ud-dev.conllu\", \"r\", encoding=\"utf-8\") \n",
    "\n",
    "eng_train_data = conllu.parse(english_train.read())  # ['id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "eng_train_sents = [[unidecode(token['form']) for token in sentence] for sentence in eng_train_data]\n",
    "eng_u_train_tags = [[token['upostag'] for token in sentence] for sentence in eng_train_data]\n",
    "\n",
    "eng_valid_data = conllu.parse(english_dev.read())  # ['id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "eng_valid_sents = [[unidecode(token['form']) for token in sentence] for sentence in eng_valid_data]\n",
    "eng_u_valid_tags = [[token['upostag'] for token in sentence] for sentence in eng_valid_data]\n",
    "\n",
    "eng_test_data = conllu.parse(english_test.read())  # ['id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "eng_test_sents = [[unidecode(token['form']) for token in sentence] for sentence in eng_test_data]\n",
    "eng_u_test_tags = [[token['upostag'] for token in sentence] for sentence in eng_test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADP',\n",
       " '_',\n",
       " 'PRON',\n",
       " 'AUX',\n",
       " 'DET',\n",
       " 'X',\n",
       " 'PUNCT',\n",
       " 'ADV',\n",
       " 'PROPN',\n",
       " 'PART',\n",
       " 'CCONJ',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'INTJ',\n",
       " 'ADJ',\n",
       " 'SCONJ',\n",
       " 'SYM',\n",
       " 'VERB']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_set = list(set(tag for tags in eng_u_train_tags for tag in tags))\n",
    "tag_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to tokenise your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use BERT to encode the tokens into contextual vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing about these pre-trained transformers is that they are normal PyTorch modules, which means that you can manipulate them however you want.\n",
    "You can get the parameters of the model, get the activations or attention values (see [here](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward) for how), and you can even optimise their parameters on your own data.\n",
    "Further optimising a pre-trained model, called fine-tuning, is done to create context vectors that work better for your data set and task.\n",
    "For example, you can use the pre-trained BERT model in your model to perform part of speech tagging by training both the softmax layer you add on top of BERT and the BERT model itself.\n",
    "This takes advantage of all the knowledge that was learned by BERT during pre-training so that you get less over-fitting and better performance on small training sets.\n",
    "\n",
    "There are certain things you need to keep in mind when fine-tuning:\n",
    "\n",
    "* You have to use `model.train(true)` and `model.train(false)` to say whether the calls you're making on the model are for optimisation or to get predictions.\n",
    "    This disables the use of dropout and other training regularisation features.\n",
    "* You should use a smaller learning rate for BERT than for your own parameters.\n",
    "    This is to avoid \"catastrophic forgetting\", which is when the pre-trained model overfits on your data and forgets what it was pre-trained on.\n",
    "    We usually use a learning rate of `2E-5` on BERT.\n",
    "    You can do this easily in PyTorch, as shown below.\n",
    "* Due to all the regularisation stuff happening inside BERT, it will have slightly unstable learning progress, which is normal, provided that the error goes down mostly.\n",
    "\n",
    "Let's look at three common uses of a BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 5-word sentence will result in 9 predictions.\n",
    "How can you make predictions for each whole word?\n",
    "\n",
    "What is usually done is that only the first token of every word is considered, with all other inner subword tokens being masked out.\n",
    "Here's a convenient function you can use in your projects that converts a list of words with tags into a list of tokens, mask, and aligned tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_tokens_and_tags(tokeniser, words, tags, no_tag=''):\n",
    "    indexes = [tokeniser.cls_token_id]\n",
    "    tag_mask = [False]\n",
    "    aligned_tags = [no_tag]\n",
    "    for tag, word in zip(tags, words):\n",
    "        for i, index in enumerate(tokeniser(word)['input_ids'][1:-1]):\n",
    "            indexes.append(index)\n",
    "            if i == 0:\n",
    "                tag_mask.append(True)\n",
    "                aligned_tags.append(tag)\n",
    "            else:\n",
    "                tag_mask.append(False)\n",
    "                aligned_tags.append(no_tag)\n",
    "    indexes.append(tokeniser.sep_token_id)\n",
    "    tag_mask.append(False)\n",
    "    aligned_tags.append(no_tag)\n",
    "    \n",
    "    return (indexes, tag_mask, aligned_tags)\n",
    "\n",
    "# words = eng_train_sents[0]\n",
    "# tags = eng_u_train_tags[0]\n",
    "# print('words:', words)\n",
    "# print('tags:', tags)\n",
    "# print()\n",
    "\n",
    "# (indexes, tag_mask, aligned_tags) = get_aligned_tokens_and_tags(tokenizer, words, tags, no_tag='_')\n",
    "# print('indexes:', indexes)\n",
    "# print('tokenised indexes:', tokenizer.convert_ids_to_tokens(indexes))\n",
    "# print('tag_mask:', tag_mask)\n",
    "# print('aligned_tags:', aligned_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then pad the sequences and turn them into a tensor using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_tokens_and_tags(all_indexes, all_tag_masks, all_aligned_tags, no_tag=''):\n",
    "    max_len = max(len(indexes) for indexes in all_indexes)\n",
    "    token_masks = []\n",
    "    new_indexes = []\n",
    "    new_tag_masks = []\n",
    "    new_aligned_tags = []\n",
    "    for (indexes, tag_mask, aligned_tags) in zip(all_indexes, all_tag_masks, all_aligned_tags):\n",
    "        num_tokens = len(indexes)\n",
    "        token_masks.append([1]*num_tokens + [0]*(max_len - num_tokens))\n",
    "        new_indexes.append(indexes + [tokenizer.pad_token_id]*(max_len - num_tokens))\n",
    "        new_tag_masks.append(tag_mask + [False]*(max_len - num_tokens))\n",
    "        new_aligned_tags.append(aligned_tags + [no_tag]*(max_len - num_tokens))\n",
    "    return (\n",
    "        torch.tensor(token_masks, dtype=torch.int64),\n",
    "        torch.tensor(new_indexes, dtype=torch.int64),\n",
    "        torch.tensor(new_tag_masks, dtype=torch.int64),\n",
    "        torch.tensor(new_aligned_tags, dtype=torch.int64),\n",
    "    )\n",
    "\n",
    "# all_words = eng_train_sents\n",
    "# all_tags = eng_u_train_tags\n",
    "\n",
    "\n",
    "# all_indexes = []\n",
    "# all_tag_masks = []\n",
    "# all_aligned_tags = []\n",
    "# for words, tags in zip(all_words, all_tags):\n",
    "#     tags = [tag_set.index(tag) for tag in tags]\n",
    "#     (indexes, tag_mask, aligned_tags) = get_aligned_tokens_and_tags(tokenizer, words, tags, no_tag=tag_set.index('PUNCT'))\n",
    "#     all_indexes.append(indexes)\n",
    "#     all_tag_masks.append(tag_mask)\n",
    "#     all_aligned_tags.append(aligned_tags)\n",
    "# (token_masks, new_indexes, new_tag_masks, new_aligned_tags) = get_padded_tokens_and_tags(all_indexes, all_tag_masks, all_aligned_tags, no_tag=tag_set.index('PUNCT'))\n",
    "\n",
    "# print('token_masks:')\n",
    "# print(token_masks)\n",
    "# print()\n",
    "# print('indexes:')\n",
    "# print(new_indexes)\n",
    "# print()\n",
    "# print('tag_mask:')\n",
    "# print(new_tag_masks)\n",
    "# print()\n",
    "# print('aligned_tags:')\n",
    "# print(new_aligned_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the code of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x:\n",
      "[['Distribution', 'of', 'this', 'license', 'does', 'not', 'create', 'an', 'attorney', '-', 'client', 'relationship', '.'], ['Creative', 'Commons', 'provides', 'this', 'information', 'on', 'an', '\"', 'as', '-', 'is', '\"', 'basis', '.'], ['Creative', 'Commons', 'makes', 'no', 'warranties', 'regarding', 'the', 'information', 'provided', ',', 'and', 'disclaims', 'liability', 'for', 'damages', 'resulting', 'from', 'its', 'use', '.'], ['License', '.'], ['The', 'work', 'is', 'protected', 'by', 'copyright', 'and', '/', 'or', 'other', 'applicable', 'law', '.']]\n",
      "\n",
      "train_y:\n",
      "[['NOUN', 'ADP', 'DET', 'NOUN', 'AUX', 'PART', 'VERB', 'DET', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT'], ['PROPN', 'PROPN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'PUNCT', 'ADP', 'PUNCT', 'VERB', 'PUNCT', 'NOUN', 'PUNCT'], ['PROPN', 'PROPN', 'VERB', 'DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'VERB', 'PUNCT', 'CCONJ', 'VERB', 'NOUN', 'ADP', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'], ['NOUN', 'PUNCT'], ['DET', 'NOUN', 'AUX', 'VERB', 'ADP', 'NOUN', 'CCONJ', 'PUNCT', 'CCONJ', 'ADJ', 'ADJ', 'NOUN', 'PUNCT']]\n",
      "\n",
      "mask_train_x:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])\n",
      "\n",
      "indexed_train_x tokens:\n",
      "['[CLS]', 'Distribution', 'of', 'this', 'license', 'does', 'not', 'create', 'an', 'attorney', '-', 'client', 'relationship', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'Creative', 'Commons', 'provides', 'this', 'information', 'on', 'an', '\"', 'as', '-', 'is', '\"', 'basis', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'Creative', 'Commons', 'makes', 'no', 'war', '##ranti', '##es', 'regarding', 'the', 'information', 'provided', ',', 'and', 'disc', '##lai', '##ms', 'li', '##ability', 'for', 'damage', '##s', 'resulting', 'from', 'its', 'use', '.', '[SEP]']\n",
      "['[CLS]', 'License', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'The', 'work', 'is', 'protected', 'by', 'copyright', 'and', '/', 'or', 'other', 'applicable', 'law', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "indexed_train_x:\n",
      "tensor([[   101,  43689,  10108,  10531,  47679,  15107,  10472,  18842,  10151,\n",
      "          54131,    118,  37748,  19808,    119,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0],\n",
      "        [   101,  23077,  13812,  20245,  10531,  12929,  10135,  10151,    107,\n",
      "          10146,    118,  10124,    107,  17172,    119,    102,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0],\n",
      "        [   101,  23077,  13812,  20562,  10192,  10338,  89106,  10171,  33295,\n",
      "          10105,  12929,  16491,    117,  10111,  27224,  31181,  12387,  11614,\n",
      "          35717,  10142,  26115,  10107,  26746,  10188,  10474,  11760,    119,\n",
      "            102],\n",
      "        [   101,  89511,    119,    102,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0],\n",
      "        [   101,  10117,  11424,  10124,  38522,  10155,  69114,  10111,    120,\n",
      "          10345,  10684, 106087,  13255,    119,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0]])\n",
      "\n",
      "mask_train_y:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "         1, 1, 1, 0],\n",
      "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])\n",
      "\n",
      "indexed_train_y tokens:\n",
      "['X', 'NOUN', 'ADP', 'DET', 'NOUN', 'AUX', 'PART', 'VERB', 'DET', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "['X', 'PROPN', 'PROPN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'PUNCT', 'ADP', 'PUNCT', 'VERB', 'PUNCT', 'NOUN', 'PUNCT', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "['X', 'PROPN', 'PROPN', 'VERB', 'DET', 'NOUN', 'X', 'X', 'VERB', 'DET', 'NOUN', 'VERB', 'PUNCT', 'CCONJ', 'VERB', 'X', 'X', 'NOUN', 'X', 'ADP', 'NOUN', 'X', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT', 'X']\n",
      "['X', 'NOUN', 'PUNCT', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "['X', 'DET', 'NOUN', 'AUX', 'VERB', 'ADP', 'NOUN', 'CCONJ', 'PUNCT', 'CCONJ', 'ADJ', 'ADJ', 'NOUN', 'PUNCT', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
      "\n",
      "indexed_train_y:\n",
      "tensor([[ 5, 12,  0,  4, 12,  3,  9, 17,  4, 12,  6, 12, 12,  6,  5,  5,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
      "        [ 5,  8,  8, 17,  4, 12,  0,  4,  6,  0,  6, 17,  6, 12,  6,  5,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
      "        [ 5,  8,  8, 17,  4, 12,  5,  5, 17,  4, 12, 17,  6, 10, 17,  5,  5, 12,\n",
      "          5,  0, 12,  5, 17,  0,  4, 12,  6,  5],\n",
      "        [ 5, 12,  6,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
      "        [ 5,  4, 12,  3, 17,  0, 12, 10,  6, 10, 14, 14, 12,  6,  5,  5,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  5]])\n"
     ]
    }
   ],
   "source": [
    "train_x = eng_train_sents[:5]\n",
    "train_y = eng_u_train_tags[:5]\n",
    "\n",
    "indexed_train_x = []\n",
    "mask_train_y = []\n",
    "indexed_train_y = []\n",
    "for (tokens, tags) in zip(train_x, train_y):\n",
    "    tags = [tag_set.index(tag) for tag in tags]\n",
    "    (indexes, tag_mask, aligned_tags) = get_aligned_tokens_and_tags(tokenizer, tokens, tags, tag_set.index('X'))\n",
    "    indexed_train_x.append(indexes)\n",
    "    mask_train_y.append(tag_mask)\n",
    "    indexed_train_y.append(aligned_tags)\n",
    "(mask_train_x, indexed_train_x, mask_train_y, indexed_train_y) = get_padded_tokens_and_tags(indexed_train_x, mask_train_y, indexed_train_y, tag_set.index('X'))\n",
    "\n",
    "print('train_x:')\n",
    "print(train_x)\n",
    "print()\n",
    "print('train_y:')\n",
    "print(train_y)\n",
    "print()\n",
    "print('mask_train_x:')\n",
    "print(mask_train_x)\n",
    "print()\n",
    "print('indexed_train_x tokens:')\n",
    "for row in indexed_train_x:\n",
    "    print(tokenizer.convert_ids_to_tokens(row))\n",
    "print()\n",
    "print('indexed_train_x:')\n",
    "print(indexed_train_x)\n",
    "print()\n",
    "print('mask_train_y:')\n",
    "print(mask_train_y)\n",
    "print()\n",
    "print('indexed_train_y tokens:')\n",
    "for row in indexed_train_y:\n",
    "    print([tag_set[index] for index in row])\n",
    "print()\n",
    "print('indexed_train_y:')\n",
    "print(indexed_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch error\n",
      "1 26.869413375854492\n",
      "2 15.400179862976074\n",
      "3 11.201945304870605\n",
      "4 7.383209228515625\n",
      "5 3.4416868686676025\n",
      "6 1.32474684715271\n",
      "7 1.790221095085144\n",
      "8 1.1491752862930298\n",
      "9 0.7319633364677429\n",
      "10 0.52610844373703\n",
      "\n",
      "sentence prediction\n",
      "['[CLS]', 'Distribution', 'of', 'this', 'license', 'does', 'not', 'create', 'an', 'attorney', '-', 'client', 'relationship', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] ['_', 'SCONJ', 'ADP', 'DET', 'NOUN', 'AUX', 'PART', 'VERB', 'DET', 'ADV', 'PUNCT', 'NOUN', 'INTJ', 'PROPN', 'SCONJ', 'INTJ', 'INTJ', 'INTJ', 'INTJ', 'INTJ', 'INTJ', 'INTJ', 'CCONJ', 'CCONJ', 'INTJ', 'SCONJ', 'ADJ', 'INTJ']\n",
      "['[CLS]', 'Creative', 'Commons', 'provides', 'this', 'information', 'on', 'an', '\"', 'as', '-', 'is', '\"', 'basis', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] ['_', 'PROPN', 'PROPN', 'VERB', 'DET', 'CCONJ', 'CCONJ', 'ADJ', 'ADV', 'ADP', 'PUNCT', 'AUX', 'PRON', 'NOUN', 'PART', 'PART', 'CCONJ', 'CCONJ', 'CCONJ', 'CCONJ', 'INTJ', 'ADJ', 'ADJ', 'ADJ', 'ADJ', 'ADJ', 'ADJ', 'SCONJ']\n",
      "['[CLS]', 'Creative', 'Commons', 'makes', 'no', 'war', '##ranti', '##es', 'regarding', 'the', 'information', 'provided', ',', 'and', 'disc', '##lai', '##ms', 'li', '##ability', 'for', 'damage', '##s', 'resulting', 'from', 'its', 'use', '.', '[SEP]'] ['_', 'PROPN', 'PROPN', 'AUX', 'PART', 'INTJ', 'SCONJ', 'SCONJ', 'ADV', 'DET', 'ADJ', 'VERB', 'PUNCT', 'CCONJ', 'VERB', 'ADJ', 'PART', 'NOUN', 'X', 'ADV', 'NOUN', 'INTJ', 'ADV', 'ADP', 'DET', 'ADV', 'ADV', 'AUX']\n",
      "['[CLS]', 'License', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] ['_', 'NOUN', 'PUNCT', 'AUX', 'VERB', 'PROPN', 'DET', 'VERB', 'SCONJ', 'DET', 'VERB', 'SCONJ', 'DET', 'DET', 'PART', 'PART', 'CCONJ', 'ADV', 'CCONJ', 'ADJ', 'PROPN', 'PROPN', 'ADJ', 'PRON', 'DET', 'DET', 'PART', 'PRON']\n",
      "['[CLS]', 'The', 'work', 'is', 'protected', 'by', 'copyright', 'and', '/', 'or', 'other', 'applicable', 'law', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] ['_', 'DET', 'NOUN', 'AUX', 'VERB', 'ADP', 'NOUN', 'ADV', 'PUNCT', 'CCONJ', 'ADV', 'ADJ', 'ADV', 'ADV', 'SCONJ', 'INTJ', 'ADV', 'PRON', 'ADV', 'ADV', 'SCONJ', 'INTJ', 'SCONJ', 'SCONJ', 'INTJ', 'SCONJ', 'SCONJ', 'SCONJ']\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, bert, num_output):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.w = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, 1.0, (768, num_output)), dtype=torch.float32))\n",
    "        self.b = torch.nn.Parameter(torch.zeros((num_output,), dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        context_vecs = self.bert(x, attention_mask=mask).last_hidden_state\n",
    "        return context_vecs@self.w + self.b\n",
    "\n",
    "model = Model(bert_model, len(tag_set))\n",
    "\n",
    "optimiser = torch.optim.Adam([{'params': [model.w, model.b], 'lr': 0.1}, {'params': bert_model.parameters(), 'lr': 2E-5}])\n",
    "\n",
    "print('epoch', 'error')\n",
    "for epoch in range(1, 10+1):\n",
    "    optimiser.zero_grad()\n",
    "    model.train(True)\n",
    "    logits = model(indexed_train_x, mask_train_x)\n",
    "    errors = torch.nn.functional.cross_entropy(logits.transpose(1, 2), indexed_train_y, reduction='none')\n",
    "    errors = errors*mask_train_y\n",
    "    error = errors.sum()/mask_train_y.sum()\n",
    "    error.backward()\n",
    "    optimiser.step()\n",
    "    model.train(False)\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(epoch, error.detach().tolist())\n",
    "\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('sentence', 'prediction')\n",
    "    logits = model(indexed_train_x, mask_train_x)\n",
    "    batch_probs = torch.softmax(logits, dim=1)\n",
    "    for (indexes, probs) in zip(indexed_train_x, batch_probs):\n",
    "        print(tokenizer.convert_ids_to_tokens(indexes), [tag_set[index] for index in probs.numpy().argmax(1).tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting hidden layer activations\n",
    "\n",
    "BERT has 12 encoder layers plus an embedding layer.\n",
    "Each one of these layers gives you a 768-element vector for each token.\n",
    "You can get the activations of each one of these layers by using the parameter `output_hidden_states=True` when calling the model.\n",
    "This will add the attribute `hidden_states` to the returned object, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "output = bert_model(tokenised_text['input_ids'], attention_mask=tokenised_text['attention_mask'], output_hidden_states=True)\n",
    "\n",
    "embeddings = output.hidden_states[0]\n",
    "layer1 = output.hidden_states[1]\n",
    "layer2 = output.hidden_states[2]\n",
    "# ...\n",
    "output_layer = output.hidden_states[12]\n",
    "\n",
    "print(embeddings.shape) # 2 sentences, 8 tokens, 768-element vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer is equivalent to calling `output.last_hidden_state` as usual.\n",
    "\n",
    "Note that the vectors at the end could be from pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  146, 1176, 1122,  119,  102,  146, 1274,  112,  189, 4819, 1122,\n",
      "          119,  102],\n",
      "        [ 101,  146, 4819, 1122,  119,  102,  146, 1274,  112,  189, 1176, 1122,\n",
      "          119,  102],\n",
      "        [ 101,  146, 1274,  112,  189, 4819, 1122,  119,  102,  146, 1176, 1122,\n",
      "          119,  102],\n",
      "        [ 101,  146, 1274,  112,  189, 1176, 1122,  119,  102,  146, 4819, 1122,\n",
      "          119,  102],\n",
      "        [ 101,  146, 1176, 1122,  119,  102,  146, 1274,  112,  189, 1176, 1122,\n",
      "          119,  102],\n",
      "        [ 101,  146, 4819, 1122,  119,  102,  146, 1274,  112,  189, 4819, 1122,\n",
      "          119,  102],\n",
      "        [ 101,  146, 1274,  112,  189, 4819, 1122,  119,  102,  146, 4819, 1122,\n",
      "          119,  102],\n",
      "        [ 101,  146, 1274,  112,  189, 1176, 1122,  119,  102,  146, 1176, 1122,\n",
      "          119,  102]])\n",
      "\n",
      "['[CLS]', 'I', 'like', 'it', '.', '[SEP]', 'I', 'don', \"'\", 't', 'hate', 'it', '.', '[SEP]']\n",
      "['[CLS]', 'I', 'hate', 'it', '.', '[SEP]', 'I', 'don', \"'\", 't', 'like', 'it', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenised_text['input_ids'])\n",
    "print()\n",
    "print(tokeniser.convert_ids_to_tokens(tokenised_text['input_ids'][0, :]))\n",
    "print(tokeniser.convert_ids_to_tokens(tokenised_text['input_ids'][1, :]))\n",
    "\n",
    "# sentence 1 activations: embeddings[0, :8, :]\n",
    "# sentence 2 activations: embeddings[0, :7, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many papers find that the middle layers, particularly layer 8, are the most transferrable across tasks.\n",
    "This means that `output.hidden_states[7]` should give a better performance when used to represent words than the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the attention values.\n",
    "BERT produces attention values in each encoder layer and has 12 attention heads.\n",
    "Remember that attention values are produced between each word pairing, making a square matrix.\n",
    "You can get the attention values in each one of these layers by using the parameter `output_attentions=True` when calling the model.\n",
    "This will add the attribute `attentions` to the returned object, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 12, 14, 14])\n",
      "attention values to produce the vector of sentence 0, head 0, word 0:\n",
      "tensor([0.4331, 0.0044, 0.0053, 0.0111, 0.0236, 0.2365, 0.0053, 0.0055, 0.0073,\n",
      "        0.0078, 0.0042, 0.0115, 0.0215, 0.2226], grad_fn=<SliceBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = bert_model(tokenised_text['input_ids'], attention_mask=tokenised_text['attention_mask'], output_attentions=True)\n",
    "layer1_attentions = output.attentions[0]\n",
    "print(layer1_attentions.shape) # 2 sentences, 12 attention heads, 8 tokens by 8 tokens\n",
    "\n",
    "print('attention values to produce the vector of sentence 0, head 0, word 0:')\n",
    "print(layer1_attentions[0, 0, 0, :]) # An attention value for each word\n",
    "print(layer1_attentions[0, 0, 0, :].sum()) # Attention values to sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if there are pad tokens then you only want to take the upper left corner of the square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence 1 attentions: layer1_attentions[0, :, :8, :8]\n",
    "# sentence 2 attentions: layer1_attentions[1, :, :7, :7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since there are many attention heads, you can take the average attention value across heads to get a nice single attention value for each word pairing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4000, 0.0300, 0.0261, 0.0363, 0.0470, 0.1200, 0.0291, 0.0196, 0.0288,\n",
      "         0.0290, 0.0317, 0.0344, 0.0461, 0.1219],\n",
      "        [0.0570, 0.0995, 0.1146, 0.0785, 0.0688, 0.1023, 0.0878, 0.0809, 0.0520,\n",
      "         0.0374, 0.0688, 0.0467, 0.0461, 0.0596],\n",
      "        [0.1009, 0.0845, 0.0601, 0.1054, 0.0748, 0.1225, 0.0571, 0.0701, 0.0552,\n",
      "         0.0400, 0.0705, 0.0422, 0.0447, 0.0718],\n",
      "        [0.0648, 0.0825, 0.0925, 0.0699, 0.1153, 0.1234, 0.0631, 0.0652, 0.0478,\n",
      "         0.0365, 0.0533, 0.0583, 0.0598, 0.0674],\n",
      "        [0.0741, 0.0428, 0.0572, 0.0809, 0.1439, 0.1896, 0.0428, 0.0327, 0.0480,\n",
      "         0.0309, 0.0328, 0.0442, 0.1189, 0.0612],\n",
      "        [0.1598, 0.0433, 0.0387, 0.0812, 0.1265, 0.1885, 0.0522, 0.0335, 0.0393,\n",
      "         0.0276, 0.0174, 0.0288, 0.0628, 0.1005],\n",
      "        [0.0464, 0.1081, 0.0535, 0.0566, 0.0757, 0.1492, 0.0843, 0.1108, 0.0649,\n",
      "         0.0399, 0.0698, 0.0395, 0.0476, 0.0537],\n",
      "        [0.1032, 0.0549, 0.0617, 0.0509, 0.0537, 0.1404, 0.0756, 0.0406, 0.1041,\n",
      "         0.0931, 0.0646, 0.0436, 0.0368, 0.0768],\n",
      "        [0.0549, 0.0409, 0.0623, 0.0474, 0.0845, 0.1196, 0.0519, 0.0622, 0.0533,\n",
      "         0.1755, 0.0530, 0.0455, 0.0760, 0.0730],\n",
      "        [0.0974, 0.0462, 0.0518, 0.0451, 0.0582, 0.0911, 0.0556, 0.1637, 0.0836,\n",
      "         0.0450, 0.0628, 0.0491, 0.0618, 0.0886],\n",
      "        [0.1336, 0.0509, 0.0587, 0.0394, 0.0410, 0.0986, 0.0655, 0.0714, 0.0591,\n",
      "         0.0497, 0.0957, 0.0978, 0.0485, 0.0901],\n",
      "        [0.0564, 0.0588, 0.0620, 0.0541, 0.0627, 0.0906, 0.0651, 0.0635, 0.0649,\n",
      "         0.0535, 0.0744, 0.0590, 0.1146, 0.1204],\n",
      "        [0.0687, 0.0369, 0.0442, 0.0410, 0.1174, 0.0664, 0.0396, 0.0333, 0.0583,\n",
      "         0.0400, 0.0415, 0.0726, 0.1449, 0.1953],\n",
      "        [0.1471, 0.0362, 0.0328, 0.0375, 0.0813, 0.1504, 0.0348, 0.0282, 0.0455,\n",
      "         0.0371, 0.0284, 0.0540, 0.1314, 0.1553]], grad_fn=<SliceBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "averaged_attentions = layer1_attentions.mean(1)\n",
    "print(averaged_attentions[0, :, :]) # Attentions in first sentence\n",
    "print(averaged_attentions[0, 0, :].sum()) # Attentions still sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
